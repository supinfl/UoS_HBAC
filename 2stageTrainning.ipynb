{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1bedec7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T00:52:39.282164Z",
     "iopub.status.busy": "2024-04-09T00:52:39.281788Z",
     "iopub.status.idle": "2024-04-09T01:18:23.924871Z",
     "shell.execute_reply": "2024-04-09T01:18:23.923838Z"
    },
    "papermill": {
     "duration": 1544.651563,
     "end_time": "2024-04-09T01:18:23.927029",
     "exception": false,
     "start_time": "2024-04-09T00:52:39.275466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 GPU(s)\n",
      "Log file path: /kaggle/working/new_version_training_record.log\n",
      "seed_everything took 0.0020 seconds.\n",
      "Train cataframe shape is: (1000, 15)\n",
      "Labels: ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
      "       eeg_id  eeg_sub_id  eeg_label_offset_seconds  spectrogram_id  \\\n",
      "0  1628180742           0                       0.0          353733   \n",
      "1  1628180742           1                       6.0          353733   \n",
      "2  1628180742           2                       8.0          353733   \n",
      "3  1628180742           3                      18.0          353733   \n",
      "4  1628180742           4                      24.0          353733   \n",
      "\n",
      "   spectrogram_sub_id  spectrogram_label_offset_seconds    label_id  \\\n",
      "0                   0                               0.0   127492639   \n",
      "1                   1                               6.0  3887563113   \n",
      "2                   2                               8.0  1142670488   \n",
      "3                   3                              18.0  2718991173   \n",
      "4                   4                              24.0  3080632009   \n",
      "\n",
      "   patient_id expert_consensus  seizure_vote  lpd_vote  gpd_vote  lrda_vote  \\\n",
      "0       42516          Seizure             3         0         0          0   \n",
      "1       42516          Seizure             3         0         0          0   \n",
      "2       42516          Seizure             3         0         0          0   \n",
      "3       42516          Seizure             3         0         0          0   \n",
      "4       42516          Seizure             3         0         0          0   \n",
      "\n",
      "   grda_vote  other_vote  \n",
      "0          0           0  \n",
      "1          0           0  \n",
      "2          0           0  \n",
      "3          0           0  \n",
      "4          0           0  \n",
      "preparing_data took 0.0235 seconds.\n",
      "Train non-overlapp eeg_id shape: (105, 12)\n",
      "      eeg_id  spectrogram_id     min     max  patient_id  seizure_vote  \\\n",
      "0    8071080         2593634     0.0    18.0        2944           0.0   \n",
      "1   72355774        11526349   112.0   112.0       40966           0.0   \n",
      "2  122762465         8440102   320.0   376.0       54724           0.0   \n",
      "3  138236967         3252414     0.0     0.0       44623           0.0   \n",
      "4  142901500        12916371  1284.0  1298.0       21996           0.0   \n",
      "\n",
      "   lpd_vote  gpd_vote  lrda_vote  grda_vote  other_vote target  \n",
      "0  0.000000  0.000000        0.0       1.00        0.00   GRDA  \n",
      "1  0.000000  0.000000        0.0       0.00        1.00  Other  \n",
      "2  0.000000  0.000000        0.0       0.25        0.75  Other  \n",
      "3  0.000000  0.000000        0.0       0.00        1.00  Other  \n",
      "4  0.166667  0.833333        0.0       0.00        0.00    GPD  \n",
      "kl value is ---      kl_divergence\n",
      "0         7.802402\n",
      "1         7.802402\n",
      "2         6.162571\n",
      "3         7.802402\n",
      "4         6.212585\n",
      "..             ...\n",
      "100       7.802402\n",
      "101       7.802402\n",
      "102       4.697900\n",
      "103       1.960275\n",
      "104       7.802402\n",
      "\n",
      "[105 rows x 1 columns]\n",
      "There are 11138 spectrogram parquets in total path\n",
      "There are 72 spectrogram parquets in this training process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [00:13<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17300 EEG spectrograms in total path\n",
      "There are 105 EEG spectrograms in this training path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:02<00:00, 40.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading_parquet took 17.6791 seconds.\n",
      "fold\n",
      "0.0    21\n",
      "1.0    21\n",
      "2.0    21\n",
      "3.0    21\n",
      "4.0    21\n",
      "dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "      eeg_id  spectrogram_id     min     max  patient_id  seizure_vote  \\\n",
      "0    8071080         2593634     0.0    18.0        2944           0.0   \n",
      "1   72355774        11526349   112.0   112.0       40966           0.0   \n",
      "2  122762465         8440102   320.0   376.0       54724           0.0   \n",
      "3  138236967         3252414     0.0     0.0       44623           0.0   \n",
      "4  142901500        12916371  1284.0  1298.0       21996           0.0   \n",
      "\n",
      "   lpd_vote  gpd_vote  lrda_vote  grda_vote  other_vote target  kl_divergence  \\\n",
      "0  0.000000  0.000000        0.0       1.00        0.00   GRDA       7.802402   \n",
      "1  0.000000  0.000000        0.0       0.00        1.00  Other       7.802402   \n",
      "2  0.000000  0.000000        0.0       0.25        0.75  Other       6.162571   \n",
      "3  0.000000  0.000000        0.0       0.00        1.00  Other       7.802402   \n",
      "4  0.166667  0.833333        0.0       0.00        0.00    GPD       6.212585   \n",
      "\n",
      "   fold  \n",
      "0   0.0  \n",
      "1   4.0  \n",
      "2   4.0  \n",
      "3   2.0  \n",
      "4   2.0  \n",
      "X shape: torch.Size([128, 256, 12])\n",
      "y shape: torch.Size([6])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade114ed90a644c3a46d8b47891b473a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/77.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Stage 1 Training for Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|█         | 1/10 [00:05<00:46,  5.18s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/10] Elapsed 0m 5s (remain 0m 46s) Loss: 1.4667 Grad: 206618.1250  LR: 0.00028000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:39<00:00,  3.98s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][9/10] Elapsed 0m 39s (remain 0m 0s) Loss: 1.4317 Grad: 153886.2188  LR: 0.00090961  \n",
      "train_epoch took 39.8329 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:07<00:15,  7.58s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 7s (remain 0m 15s) Loss: 1.1072 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:26<00:00,  8.93s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 26s (remain 0m 0s) Loss: 1.0488 \n",
      "valid_epoch took 26.8023 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  10%|█         | 1/10 [00:03<00:32,  3.63s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/10] Elapsed 0m 3s (remain 0m 32s) Loss: 1.1001 Grad: 110346.8984  LR: 0.00088307  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:38<00:00,  3.80s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][9/10] Elapsed 0m 38s (remain 0m 0s) Loss: 1.1805 Grad: 45817.0039  LR: 0.00054376  \n",
      "train_epoch took 38.0544 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:07<00:15,  7.51s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 7s (remain 0m 15s) Loss: 1.0011 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:26<00:00,  8.88s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 26s (remain 0m 0s) Loss: 1.0399 \n",
      "valid_epoch took 26.6373 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  10%|█         | 1/10 [00:03<00:32,  3.59s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/10] Elapsed 0m 3s (remain 0m 32s) Loss: 0.9174 Grad: 186931.7656  LR: 0.00050020  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:38<00:00,  3.80s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][9/10] Elapsed 0m 38s (remain 0m 0s) Loss: 0.9514 Grad: 105155.8672  LR: 0.00014679  \n",
      "train_epoch took 38.0227 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:07<00:15,  7.60s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 7s (remain 0m 15s) Loss: 1.0417 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:26<00:00,  8.92s/valid_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 26s (remain 0m 0s) Loss: 1.1213 \n",
      "valid_epoch took 26.7746 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|█         | 1/10 [00:03<00:32,  3.61s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/10] Elapsed 0m 3s (remain 0m 32s) Loss: 0.8237 Grad: 91463.0781  LR: 0.00011733  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:38<00:00,  3.81s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][9/10] Elapsed 0m 38s (remain 0m 0s) Loss: 0.8358 Grad: 57273.0039  LR: 0.00000230  \n",
      "train_epoch took 38.0934 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:07<00:15,  7.53s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 7s (remain 0m 15s) Loss: 1.0844 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:26<00:00,  8.87s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 26s (remain 0m 0s) Loss: 1.1510 \n",
      "valid_epoch took 26.6175 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loop took 262.6432 seconds.\n",
      "Starting Stage 2 Training for Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  50%|█████     | 1/2 [00:04<00:04,  4.74s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/2] Elapsed 0m 4s (remain 0m 4s) Loss: 0.9491 Grad: 116464.1406  LR: 0.00093304  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2/2 [00:08<00:00,  4.24s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1/2] Elapsed 0m 8s (remain 0m 0s) Loss: 1.0828 Grad: 423023.2188  LR: 0.00078687  \n",
      "train_epoch took 8.4908 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:07<00:15,  7.63s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 7s (remain 0m 15s) Loss: 1.2465 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:26<00:00,  8.98s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 26s (remain 0m 0s) Loss: 1.1605 \n",
      "valid_epoch took 26.9373 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  50%|█████     | 1/2 [00:04<00:04,  4.76s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/2] Elapsed 0m 4s (remain 0m 4s) Loss: 0.6344 Grad: 153091.3594  LR: 0.00058699  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2/2 [00:08<00:00,  4.27s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1/2] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7345 Grad: 94386.0391  LR: 0.00037084  \n",
      "train_epoch took 8.5487 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:07<00:15,  7.55s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 7s (remain 0m 15s) Loss: 1.3456 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:26<00:00,  8.88s/valid_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 26s (remain 0m 0s) Loss: 1.2279 \n",
      "valid_epoch took 26.6629 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  50%|█████     | 1/2 [00:04<00:04,  4.80s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/2] Elapsed 0m 4s (remain 0m 4s) Loss: 0.6809 Grad: 122046.0703  LR: 0.00017893  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2/2 [00:08<00:00,  4.29s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1/2] Elapsed 0m 8s (remain 0m 0s) Loss: 0.7705 Grad: 93925.9297  LR: 0.00004723  \n",
      "train_epoch took 8.5869 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:07<00:15,  7.57s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 7s (remain 0m 15s) Loss: 1.3355 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:26<00:00,  8.94s/valid_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 26s (remain 0m 0s) Loss: 1.2188 \n",
      "valid_epoch took 26.8372 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  50%|█████     | 1/2 [00:04<00:04,  4.76s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/2] Elapsed 0m 4s (remain 0m 4s) Loss: 0.6304 Grad: 98058.7812  LR: 0.00000040  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2/2 [00:08<00:00,  4.27s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1/2] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6995 Grad: 134640.2812  LR: 0.00004723  \n",
      "train_epoch took 8.5429 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:07<00:15,  7.55s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 7s (remain 0m 15s) Loss: 1.2572 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:26<00:00,  8.96s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 26s (remain 0m 0s) Loss: 1.1585 \n",
      "valid_epoch took 26.8771 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loop took 143.2504 seconds.\n",
      "Starting Stage 1 Training for Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|█         | 1/10 [00:03<00:33,  3.69s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/10] Elapsed 0m 3s (remain 0m 33s) Loss: 1.4224 Grad: 142572.5469  LR: 0.00028000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:48<00:00,  4.82s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][9/10] Elapsed 0m 48s (remain 0m 0s) Loss: 1.2989 Grad: 185577.5469  LR: 0.00090961  \n",
      "train_epoch took 48.1980 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.02s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.6232 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.04valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.6218 \n",
      "valid_epoch took 2.8966 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  10%|█         | 1/10 [00:03<00:33,  3.70s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/10] Elapsed 0m 3s (remain 0m 33s) Loss: 1.0969 Grad: 115692.5078  LR: 0.00088307  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:47<00:00,  4.79s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][9/10] Elapsed 0m 47s (remain 0m 0s) Loss: 1.0214 Grad: 98284.5703  LR: 0.00054376  \n",
      "train_epoch took 47.9356 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.01s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 2.2842 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.06valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.9216 \n",
      "valid_epoch took 2.8308 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|█         | 1/10 [00:03<00:32,  3.62s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/10] Elapsed 0m 3s (remain 0m 32s) Loss: 1.1530 Grad: 200647.2812  LR: 0.00050020  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:47<00:00,  4.79s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][9/10] Elapsed 0m 47s (remain 0m 0s) Loss: 0.9097 Grad: 122259.9297  LR: 0.00014679  \n",
      "train_epoch took 47.8837 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:00<00:01,  1.01valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 0s (remain 0m 1s) Loss: 2.7491 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.07valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 2.1397 \n",
      "valid_epoch took 2.8105 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|█         | 1/10 [00:03<00:32,  3.65s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/10] Elapsed 0m 3s (remain 0m 32s) Loss: 0.9102 Grad: 90855.3203  LR: 0.00011733  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:47<00:00,  4.78s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][9/10] Elapsed 0m 47s (remain 0m 0s) Loss: 0.7851 Grad: 149457.5000  LR: 0.00000230  \n",
      "train_epoch took 47.8491 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:00<00:01,  1.01valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 0s (remain 0m 1s) Loss: 2.6892 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.08valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 2.0982 \n",
      "valid_epoch took 2.7997 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loop took 204.5692 seconds.\n",
      "Starting Stage 2 Training for Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  33%|███▎      | 1/3 [00:05<00:10,  5.34s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/3] Elapsed 0m 5s (remain 0m 10s) Loss: 0.8371 Grad: 92562.7812  LR: 0.00098653  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:16<00:00,  5.45s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2/3] Elapsed 0m 16s (remain 0m 0s) Loss: 0.8385 Grad: 94097.5156  LR: 0.00084318  \n",
      "train_epoch took 16.3463 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:00<00:01,  1.01valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 0s (remain 0m 1s) Loss: 1.5496 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.08valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.5809 \n",
      "valid_epoch took 2.7940 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  33%|███▎      | 1/3 [00:05<00:10,  5.25s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/3] Elapsed 0m 5s (remain 0m 10s) Loss: 0.7245 Grad: 96759.8203  LR: 0.00072451  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:16<00:00,  5.45s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2/3] Elapsed 0m 16s (remain 0m 0s) Loss: 0.6527 Grad: 130439.3828  LR: 0.00044218  \n",
      "train_epoch took 16.3474 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:00<00:01,  1.01valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 0s (remain 0m 1s) Loss: 1.5294 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.08valid_batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.5857 \n",
      "valid_epoch took 2.7992 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  33%|███▎      | 1/3 [00:05<00:10,  5.26s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/3] Elapsed 0m 5s (remain 0m 10s) Loss: 0.5869 Grad: 96894.1016  LR: 0.00030224  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:16<00:00,  5.42s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][2/3] Elapsed 0m 16s (remain 0m 0s) Loss: 0.5577 Grad: 202016.4688  LR: 0.00008262  \n",
      "train_epoch took 16.2798 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:00<00:01,  1.00valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 0s (remain 0m 1s) Loss: 1.5227 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.07valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.5797 \n",
      "valid_epoch took 2.8087 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  33%|███▎      | 1/3 [00:05<00:10,  5.26s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/3] Elapsed 0m 5s (remain 0m 10s) Loss: 0.5200 Grad: 65304.5508  LR: 0.00002140  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:16<00:00,  5.43s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][2/3] Elapsed 0m 16s (remain 0m 0s) Loss: 0.5445 Grad: 129407.7031  LR: 0.00002140  \n",
      "train_epoch took 16.2956 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:00<00:01,  1.01valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 0s (remain 0m 1s) Loss: 1.5504 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.07valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.5869 \n",
      "valid_epoch took 2.8143 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loop took 78.1571 seconds.\n",
      "Starting Stage 1 Training for Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|█         | 1/10 [00:04<00:36,  4.11s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/10] Elapsed 0m 4s (remain 0m 36s) Loss: 1.4505 Grad: 175500.2500  LR: 0.00028000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:47<00:00,  4.75s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][9/10] Elapsed 0m 47s (remain 0m 0s) Loss: 1.3217 Grad: inf  LR: 0.00090961  \n",
      "train_epoch took 47.4818 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:03,  1.91s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 3s) Loss: 1.5187 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.26s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.5496 \n",
      "valid_epoch took 3.7768 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  10%|█         | 1/10 [00:04<00:37,  4.15s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/10] Elapsed 0m 4s (remain 0m 37s) Loss: 1.3899 Grad: 246738.6875  LR: 0.00088307  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:47<00:00,  4.74s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][9/10] Elapsed 0m 47s (remain 0m 0s) Loss: 1.1376 Grad: inf  LR: 0.00054376  \n",
      "train_epoch took 47.3677 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:03,  1.90s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 3s) Loss: 1.3804 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.25s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.4826 \n",
      "valid_epoch took 3.7539 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  10%|█         | 1/10 [00:04<00:37,  4.13s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/10] Elapsed 0m 4s (remain 0m 37s) Loss: 1.0636 Grad: 169333.2188  LR: 0.00050020  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:47<00:00,  4.74s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][9/10] Elapsed 0m 47s (remain 0m 0s) Loss: 0.9904 Grad: 268612.7500  LR: 0.00014679  \n",
      "train_epoch took 47.4581 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:03,  1.90s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 3s) Loss: 1.4377 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.24s/valid_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.5285 \n",
      "valid_epoch took 3.7183 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|█         | 1/10 [00:04<00:37,  4.13s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/10] Elapsed 0m 4s (remain 0m 37s) Loss: 1.0540 Grad: 182006.5156  LR: 0.00011733  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:47<00:00,  4.75s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][9/10] Elapsed 0m 47s (remain 0m 0s) Loss: 0.9083 Grad: 253590.6562  LR: 0.00000230  \n",
      "train_epoch took 47.4699 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:03,  1.90s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 3s) Loss: 1.3991 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.24s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.5260 \n",
      "valid_epoch took 3.7266 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loop took 206.3971 seconds.\n",
      "Starting Stage 2 Training for Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  33%|███▎      | 1/3 [00:04<00:08,  4.37s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/3] Elapsed 0m 4s (remain 0m 8s) Loss: 0.8854 Grad: 105124.7500  LR: 0.00098653  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:15<00:00,  5.14s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2/3] Elapsed 0m 15s (remain 0m 0s) Loss: 0.8628 Grad: 134195.4062  LR: 0.00084318  \n",
      "train_epoch took 15.4208 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:03,  1.91s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 3s) Loss: 1.6445 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.24s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.6012 \n",
      "valid_epoch took 3.7407 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  33%|███▎      | 1/3 [00:04<00:08,  4.33s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/3] Elapsed 0m 4s (remain 0m 8s) Loss: 0.7738 Grad: 92117.0703  LR: 0.00072451  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:15<00:00,  5.12s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2/3] Elapsed 0m 15s (remain 0m 0s) Loss: 0.7012 Grad: 308892.4375  LR: 0.00044218  \n",
      "train_epoch took 15.3679 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:03,  1.89s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 3s) Loss: 1.6352 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.24s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.5933 \n",
      "valid_epoch took 3.7197 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  33%|███▎      | 1/3 [00:04<00:08,  4.28s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/3] Elapsed 0m 4s (remain 0m 8s) Loss: 0.7874 Grad: 233918.0312  LR: 0.00030224  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:15<00:00,  5.10s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][2/3] Elapsed 0m 15s (remain 0m 0s) Loss: 0.6496 Grad: 120171.8125  LR: 0.00008262  \n",
      "train_epoch took 15.3058 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:03,  1.89s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 3s) Loss: 1.5306 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.25s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.5466 \n",
      "valid_epoch took 3.7633 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  33%|███▎      | 1/3 [00:04<00:08,  4.27s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/3] Elapsed 0m 4s (remain 0m 8s) Loss: 0.6670 Grad: 160364.0156  LR: 0.00002140  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:15<00:00,  5.10s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][2/3] Elapsed 0m 15s (remain 0m 0s) Loss: 0.6020 Grad: 88116.0625  LR: 0.00002140  \n",
      "train_epoch took 15.2975 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:03,  1.89s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 3s) Loss: 1.4748 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.23s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.5344 \n",
      "valid_epoch took 3.7109 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loop took 78.5049 seconds.\n",
      "Starting Stage 1 Training for Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|█         | 1/10 [00:03<00:32,  3.57s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/10] Elapsed 0m 3s (remain 0m 32s) Loss: 1.6172 Grad: 211082.9531  LR: 0.00028000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:48<00:00,  4.81s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][9/10] Elapsed 0m 48s (remain 0m 0s) Loss: 1.4223 Grad: 149608.7812  LR: 0.00090961  \n",
      "train_epoch took 48.1594 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.3724 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.13valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.4049 \n",
      "valid_epoch took 2.6594 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  10%|█         | 1/10 [00:03<00:32,  3.56s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/10] Elapsed 0m 3s (remain 0m 32s) Loss: 1.4219 Grad: 156934.7344  LR: 0.00088307  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:48<00:00,  4.84s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][9/10] Elapsed 0m 48s (remain 0m 0s) Loss: 1.2340 Grad: 118126.0000  LR: 0.00054376  \n",
      "train_epoch took 48.4053 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.24s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.1324 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.12valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.1629 \n",
      "valid_epoch took 2.6924 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  10%|█         | 1/10 [00:03<00:32,  3.56s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/10] Elapsed 0m 3s (remain 0m 32s) Loss: 1.2796 Grad: 138379.3750  LR: 0.00050020  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:48<00:00,  4.83s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][9/10] Elapsed 0m 48s (remain 0m 0s) Loss: 1.0854 Grad: 181087.3750  LR: 0.00014679  \n",
      "train_epoch took 48.3438 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 0.9970 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.12valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.0397 \n",
      "valid_epoch took 2.6915 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  10%|█         | 1/10 [00:03<00:32,  3.59s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/10] Elapsed 0m 3s (remain 0m 32s) Loss: 1.0189 Grad: 137235.6250  LR: 0.00011733  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:48<00:00,  4.85s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][9/10] Elapsed 0m 48s (remain 0m 0s) Loss: 0.9537 Grad: 113226.3203  LR: 0.00000230  \n",
      "train_epoch took 48.5006 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.26s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.0151 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.11valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.0330 \n",
      "valid_epoch took 2.7179 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loop took 206.2917 seconds.\n",
      "Starting Stage 2 Training for Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  50%|█████     | 1/2 [00:05<00:05,  5.20s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/2] Elapsed 0m 5s (remain 0m 5s) Loss: 0.9896 Grad: 106951.9844  LR: 0.00093304  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2/2 [00:08<00:00,  4.49s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1/2] Elapsed 0m 8s (remain 0m 0s) Loss: 0.8994 Grad: 80132.1719  LR: 0.00078687  \n",
      "train_epoch took 8.9836 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.25s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.4141 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.13valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.4495 \n",
      "valid_epoch took 2.6728 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  50%|█████     | 1/2 [00:05<00:05,  5.28s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/2] Elapsed 0m 5s (remain 0m 5s) Loss: 0.7780 Grad: 82517.6250  LR: 0.00058699  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2/2 [00:09<00:00,  4.53s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1/2] Elapsed 0m 9s (remain 0m 0s) Loss: 0.7726 Grad: 139852.3750  LR: 0.00037084  \n",
      "train_epoch took 9.0749 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.25s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.3904 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.13valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.4212 \n",
      "valid_epoch took 2.6761 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  50%|█████     | 1/2 [00:05<00:05,  5.19s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/2] Elapsed 0m 5s (remain 0m 5s) Loss: 0.6291 Grad: 119349.8750  LR: 0.00017893  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2/2 [00:08<00:00,  4.48s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1/2] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6348 Grad: 54733.8164  LR: 0.00004723  \n",
      "train_epoch took 8.9778 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.25s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.3775 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.12valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.4099 \n",
      "valid_epoch took 2.6962 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  50%|█████     | 1/2 [00:05<00:05,  5.21s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/2] Elapsed 0m 5s (remain 0m 5s) Loss: 0.6409 Grad: 109681.5156  LR: 0.00000040  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 2/2 [00:08<00:00,  4.49s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1/2] Elapsed 0m 8s (remain 0m 0s) Loss: 0.6655 Grad: 116503.3438  LR: 0.00004723  \n",
      "train_epoch took 8.9957 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.25s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.3563 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:02<00:00,  1.13valid_batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 2s (remain 0m 0s) Loss: 1.3900 \n",
      "valid_epoch took 2.6720 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loop took 48.9102 seconds.\n",
      "Starting Stage 1 Training for Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  10%|█         | 1/10 [00:03<00:33,  3.71s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/10] Elapsed 0m 3s (remain 0m 33s) Loss: 1.5316 Grad: 143364.2812  LR: 0.00028000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:47<00:00,  4.79s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][9/10] Elapsed 0m 47s (remain 0m 0s) Loss: 1.3478 Grad: 201270.2812  LR: 0.00090961  \n",
      "train_epoch took 47.9233 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.46s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.3229 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.00s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.4356 \n",
      "valid_epoch took 3.0193 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  10%|█         | 1/10 [00:03<00:33,  3.69s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/10] Elapsed 0m 3s (remain 0m 33s) Loss: 1.3645 Grad: 151697.5000  LR: 0.00088307  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:48<00:00,  4.80s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][9/10] Elapsed 0m 48s (remain 0m 0s) Loss: 1.2123 Grad: 105433.6953  LR: 0.00054376  \n",
      "train_epoch took 48.0438 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.46s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.1546 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.01s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.2480 \n",
      "valid_epoch took 3.0341 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  10%|█         | 1/10 [00:03<00:33,  3.70s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/10] Elapsed 0m 3s (remain 0m 33s) Loss: 1.1716 Grad: 262954.1562  LR: 0.00050020  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:48<00:00,  4.82s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][9/10] Elapsed 0m 48s (remain 0m 0s) Loss: 0.9663 Grad: 111703.1641  LR: 0.00014679  \n",
      "train_epoch took 48.1979 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.47s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.0177 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.02s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.0893 \n",
      "valid_epoch took 3.0565 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  10%|█         | 1/10 [00:03<00:33,  3.72s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/10] Elapsed 0m 3s (remain 0m 33s) Loss: 0.9900 Grad: 126916.1484  LR: 0.00011733  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 10/10 [00:48<00:00,  4.81s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][9/10] Elapsed 0m 48s (remain 0m 0s) Loss: 0.8730 Grad: 131830.5625  LR: 0.00000230  \n",
      "train_epoch took 48.0652 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.46s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.0120 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.01s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.0692 \n",
      "valid_epoch took 3.0372 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loop took 206.4848 seconds.\n",
      "Starting Stage 2 Training for Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:  33%|███▎      | 1/3 [00:05<00:10,  5.29s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/3] Elapsed 0m 5s (remain 0m 10s) Loss: 0.8146 Grad: 108176.0234  LR: 0.00098653  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:16<00:00,  5.45s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2/3] Elapsed 0m 16s (remain 0m 0s) Loss: 0.8390 Grad: 153540.3281  LR: 0.00084318  \n",
      "train_epoch took 16.3651 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.50s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 3s) Loss: 1.3761 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.02s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.5077 \n",
      "valid_epoch took 3.0765 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  33%|███▎      | 1/3 [00:05<00:10,  5.15s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/3] Elapsed 0m 5s (remain 0m 10s) Loss: 0.7815 Grad: 372374.2812  LR: 0.00072451  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:16<00:00,  5.43s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2/3] Elapsed 0m 16s (remain 0m 0s) Loss: 0.7159 Grad: 108057.5391  LR: 0.00044218  \n",
      "train_epoch took 16.2995 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.47s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.3777 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.01s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.4822 \n",
      "valid_epoch took 3.0529 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  33%|███▎      | 1/3 [00:05<00:10,  5.19s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/3] Elapsed 0m 5s (remain 0m 10s) Loss: 0.6353 Grad: 88941.9531  LR: 0.00030224  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:16<00:00,  5.41s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][2/3] Elapsed 0m 16s (remain 0m 0s) Loss: 0.6344 Grad: 209431.8594  LR: 0.00008262  \n",
      "train_epoch took 16.2483 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.47s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.3197 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.01s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.4112 \n",
      "valid_epoch took 3.0413 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Train:  33%|███▎      | 1/3 [00:05<00:10,  5.24s/train_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/3] Elapsed 0m 5s (remain 0m 10s) Loss: 0.7093 Grad: 134372.3750  LR: 0.00002140  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3/3 [00:16<00:00,  5.43s/train_batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][2/3] Elapsed 0m 16s (remain 0m 0s) Loss: 0.6916 Grad: 220671.8438  LR: 0.00002140  \n",
      "train_epoch took 16.3005 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:  33%|███▎      | 1/3 [00:01<00:02,  1.48s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [0/3] Elapsed 0m 1s (remain 0m 2s) Loss: 1.2667 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 3/3 [00:03<00:00,  1.02s/valid_batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [2/3] Elapsed 0m 3s (remain 0m 0s) Loss: 1.3548 \n",
      "valid_epoch took 3.0642 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loop took 79.6918 seconds.\n",
      "get_result took 0.0043 seconds.\n",
      "========== CV: 1.4233845923937047 ==========\n",
      "get_result took 0.0012 seconds.\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from glob import glob\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "from skimage.transform import resize\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import functools\n",
    "import pywt\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Using', torch.cuda.device_count(), 'GPU(s)')\n",
    "\n",
    "\n",
    "\n",
    "class config:\n",
    "    AMP = True\n",
    "    BATCH_SIZE_TRAIN = 8\n",
    "    BATCH_SIZE_VALID = 8\n",
    "    EPOCHS = 4\n",
    "    FOLDS = 5\n",
    "    FREEZE = False\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    MAX_GRAD_NORM = 1e7\n",
    "    MODEL = \"efficientnet_b4\"\n",
    "    NUM_FROZEN_LAYERS = 39\n",
    "    NUM_WORKERS = 0 # multiprocessing.cpu_count()\n",
    "    PRINT_FREQ = 20\n",
    "    SEED = 20\n",
    "    TRAIN_FULL_DATA = False\n",
    "    VISUALIZE = False\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    \n",
    "    \n",
    "class paths:\n",
    "    OUTPUT_DIR = Path(\"/kaggle/working/\")\n",
    "#     PRE_LOADED_EEGS = '/kaggle/input/brain-eeg-spectrograms/eeg_specs.npy'\n",
    "#     PRE_LOADED_SPECTROGRAMS = '/kaggle/input/brain-spectrograms/specs.npy'\n",
    "    TRAIN_CSV = \"/kaggle/input/hms-harmful-brain-activity-classification/train.csv\"\n",
    "    TRAIN_EEGS = \"/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/\"\n",
    "    TRAIN_SPECTROGRAMS = \"/kaggle/input/hms-harmful-brain-activity-classification/train_spectrograms/\"\n",
    "    ROOT = Path.cwd()\n",
    "    INPUT = ROOT / \"input\"\n",
    "    # OUTPUT_DIR = ROOT / \"output\"\n",
    "    DATA = Path(\"./original_data\")\n",
    "    \n",
    "    PRE_LOADED_EEGS = '/kaggle/working/brain-eeg-spectrograms/eeg_specs.npy'\n",
    "    PRE_LOADED_SPECTROGRAMS = '/kaggle/brain-spectrograms/working/specs.npy'\n",
    "    PRE_LOADED_Wavelets = '/kaggle/working/brain-wavelets/specs.npy'\n",
    "    \n",
    "    # PRE_LOADED_EEGS = './kaggle/input/brain-eeg-spectrograms/eeg_specs.npy'\n",
    "    # PRE_LOADED_SPECTROGRAMS = './kaggle/input/brain-spectrograms/specs.npy'\n",
    "#     PRE_LOADED_Wavelets = './kaggle/input/brain-wavelets/specs.npy'\n",
    "    \n",
    "    # TRAIN_SPECTROGRAMS = DATA / \"train_spectrograms\"\n",
    "    # TRAIN_EEGS = DATA / \"train_eegs\"\n",
    "    # TRAIN_CSV = DATA / \"train.csv\"\n",
    "\n",
    "log_filename = paths.ROOT/'new_version_training_record.log'\n",
    "\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "def log_time(func):\n",
    "    \"\"\"warpper for logging running time\"\"\"\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        logging.info(f\"{func.__name__} took {end_time - start_time:.4f} seconds.\")\n",
    "        print(f\"{func.__name__} took {end_time - start_time:.4f} seconds.\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s: float):\n",
    "    \"Convert to minutes.\"\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since: float, percent: float):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "\n",
    "def plot_spectrogram(spectrogram_path: str):\n",
    "    \"\"\"\n",
    "    Source: https://www.kaggle.com/code/mvvppp/hms-eda-and-domain-journey\n",
    "    Visualize spectrogram recordings from a parquet file.\n",
    "    :param spectrogram_path: path to the spectrogram parquet.\n",
    "    \"\"\"\n",
    "    sample_spect = pd.read_parquet(spectrogram_path)\n",
    "    \n",
    "    split_spect = {\n",
    "        \"LL\": sample_spect.filter(regex='^LL', axis=1),\n",
    "        \"RL\": sample_spect.filter(regex='^RL', axis=1),\n",
    "        \"RP\": sample_spect.filter(regex='^RP', axis=1),\n",
    "        \"LP\": sample_spect.filter(regex='^LP', axis=1),\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    label_interval = 5\n",
    "    for i, split_name in enumerate(split_spect.keys()):\n",
    "        ax = axes[i]\n",
    "        img = ax.imshow(np.log(split_spect[split_name]).T, cmap='viridis', aspect='auto', origin='lower')\n",
    "        cbar = fig.colorbar(img, ax=ax)\n",
    "        cbar.set_label('Log(Value)')\n",
    "        ax.set_title(split_name)\n",
    "        ax.set_ylabel(\"Frequency (Hz)\")\n",
    "        ax.set_xlabel(\"Time\")\n",
    "\n",
    "        ax.set_yticks(np.arange(len(split_spect[split_name].columns)))\n",
    "        ax.set_yticklabels([column_name[3:] for column_name in split_spect[split_name].columns])\n",
    "        frequencies = [column_name[3:] for column_name in split_spect[split_name].columns]\n",
    "        ax.set_yticks(np.arange(0, len(split_spect[split_name].columns), label_interval))\n",
    "        ax.set_yticklabels(frequencies[::label_interval])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "@log_time   \n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n",
    "\n",
    "   \n",
    "def sep():\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, df: pd.DataFrame, config,\n",
    "        augment: bool = False, mode: str = 'train',\n",
    "        specs: Dict[int, np.ndarray] = None,\n",
    "        eeg_specs: Dict[int, np.ndarray] = None,\n",
    "        wavelets_spectrograms: Dict[int, np.ndarray] = None,\n",
    "    ): \n",
    "        self.df = df\n",
    "        self.config = config\n",
    "        self.batch_size = self.config.BATCH_SIZE_TRAIN\n",
    "        self.augment = augment\n",
    "        self.mode = mode\n",
    "        self.spectrograms = specs if specs is not None else {}\n",
    "        self.eeg_spectrograms = eeg_specs if eeg_specs is not None else {}\n",
    "        self.wavelets_spectrograms = wavelets_spectrograms if wavelets_spectrograms is not None else {}\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the number of batches per epoch.\n",
    "        \"\"\"\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one batch of data.\n",
    "        \"\"\"\n",
    "        X, y = self.__data_generation(index)\n",
    "        if self.augment:\n",
    "            X = self.__transform(X) \n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def log_and_Standarize(self,img):\n",
    "        # Log transform spectrogram\n",
    "            img = np.clip(img, np.exp(-4), np.exp(8))\n",
    "            img = np.log(img)\n",
    "\n",
    "            # Standarize per image\n",
    "            ep = 1e-6\n",
    "            mu = np.nanmean(img.flatten())\n",
    "            std = np.nanstd(img.flatten())\n",
    "            img = (img - mu) / (std + ep)\n",
    "            img = np.nan_to_num(img, nan=0.0)\n",
    "            return img\n",
    "\n",
    "    def __data_generation(self, index):\n",
    "        \"\"\"\n",
    "        Generates data containing batch_size samples.\n",
    "        \"\"\"\n",
    "        X = np.zeros((128, 256, 12), dtype='float32')\n",
    "        y = np.zeros(6, dtype='float32')\n",
    "        img = np.ones((128,256), dtype='float32')\n",
    "        row = self.df.iloc[index]\n",
    "        if self.mode=='test': \n",
    "            r = 0\n",
    "        else: \n",
    "            r = int((row['min'] + row['max']) // 4)\n",
    "            \n",
    "        for region in range(4):\n",
    "            img = self.spectrograms[row.spectrogram_id][r:r+300, region*100:(region+1)*100].T\n",
    "            img = self.log_and_Standarize(img)\n",
    "            X[14:-14, :, region] = img[:, 22:-22] / 2.0\n",
    "            \n",
    "        img = self.eeg_spectrograms[row.eeg_id]\n",
    "        img = img.to_numpy()\n",
    "        img = self.log_and_Standarize(img)\n",
    "        img = resize(img, (128, 256, 4))\n",
    "        X[:, :, 4:8] = img\n",
    "\n",
    "        # Combine wavelet features\n",
    "        img = self.wavelets_spectrograms[row.spectrogram_id]\n",
    "        img = self.log_and_Standarize(img)\n",
    "        img = resize(img, (128, 256,4))\n",
    "        X[:, :, 8:12] = img\n",
    "\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            y = row[label_cols].values.astype(np.float32)\n",
    "    \n",
    "        return X, y\n",
    "    \n",
    "    def __transform(self, img):\n",
    "        transforms = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "        ])\n",
    "        return transforms(image=img)['image']\n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, config, num_classes: int = 6, pretrained: bool = True):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.USE_KAGGLE_SPECTROGRAMS = True\n",
    "        self.USE_EEG_SPECTROGRAMS = True\n",
    "        self.USE_WAVELET_SPECTROGRAMS = True\n",
    "        self.model = timm.create_model(\n",
    "            config.MODEL,\n",
    "            pretrained=pretrained,\n",
    "            drop_rate = 0.1,\n",
    "            drop_path_rate = 0.2,\n",
    "        )\n",
    "        if config.FREEZE:\n",
    "            for i,(name, param) in enumerate(list(self.model.named_parameters())\\\n",
    "                                             [0:config.NUM_FROZEN_LAYERS]):\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.features = nn.Sequential(*list(self.model.children())[:-2])\n",
    "        self.custom_layers = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.model.num_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def __reshape_input(self, x):\n",
    "        \"\"\"\n",
    "        Reshapes input torch.Size([8, 128, 256, 12]) -> [8, 3, 512, 768] monotone image.\n",
    "        \"\"\" \n",
    "        components = []\n",
    "        if self.USE_KAGGLE_SPECTROGRAMS:\n",
    "            spectrograms = [x[:, :, :, i:i+1] for i in range(4)]\n",
    "            components.append(torch.cat(spectrograms, dim=1))\n",
    "        if self.USE_EEG_SPECTROGRAMS:\n",
    "            eegs = [x[:, :, :, i:i+1] for i in range(4,8)]\n",
    "            eegs = torch.cat(eegs, dim=1)\n",
    "            components.append(eegs)\n",
    "\n",
    "        if self.USE_WAVELET_SPECTROGRAMS:\n",
    "            wavelets = [x[:, :, :, i:i+1] for i in range(8,12)]\n",
    "            wavelets = torch.cat(wavelets, dim=1)\n",
    "            components.append(wavelets)\n",
    "\n",
    "        if components:\n",
    "            x = torch.cat(components, dim=2)\n",
    "\n",
    "        x = torch.cat([x, x, x], dim=3)  \n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.__reshape_input(x)\n",
    "        x = self.features(x)\n",
    "        x = self.custom_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@log_time\n",
    "def train_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    \"\"\"One epoch training pass.\"\"\"\n",
    "    model.train() \n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=config.AMP)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    \n",
    "    # ========== ITERATE OVER TRAIN BATCHES ============\n",
    "    with tqdm(train_loader, unit=\"train_batch\", desc='Train') as tqdm_train_loader:\n",
    "        for step, (X, y) in enumerate(tqdm_train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = y.size(0)\n",
    "            with torch.cuda.amp.autocast(enabled=config.AMP):\n",
    "                y_preds = model(X) \n",
    "                loss = criterion(F.log_softmax(y_preds, dim=1), y)\n",
    "            if config.GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "                loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            scaler.scale(loss).backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
    "\n",
    "            if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                scheduler.step()\n",
    "            end = time.time()\n",
    "\n",
    "            # ========== LOG INFO ==========\n",
    "            if step % config.PRINT_FREQ == 0 or step == (len(train_loader)-1):\n",
    "                print('Epoch: [{0}][{1}/{2}] '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.avg:.4f} '\n",
    "                      'Grad: {grad_norm:.4f}  '\n",
    "                      'LR: {lr:.8f}  '\n",
    "                      .format(epoch+1, step, len(train_loader), \n",
    "                              remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                              loss=losses,\n",
    "                              grad_norm=grad_norm,\n",
    "                              lr=scheduler.get_last_lr()[0]))\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "@log_time\n",
    "def valid_epoch(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    losses = AverageMeter()\n",
    "    prediction_dict = {}\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    with tqdm(valid_loader, unit=\"valid_batch\", desc='Validation') as tqdm_valid_loader:\n",
    "        for step, (X, y) in enumerate(tqdm_valid_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = y.size(0)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(X)\n",
    "                loss = criterion(F.log_softmax(y_preds, dim=1), y)\n",
    "            if config.GRADIENT_ACCUMULATION_STEPS > 1:\n",
    "                loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            y_preds = softmax(y_preds)\n",
    "            preds.append(y_preds.to('cpu').numpy())\n",
    "            end = time.time()\n",
    "\n",
    "            # ========== LOG INFO ==========\n",
    "            if step % config.PRINT_FREQ == 0 or step == (len(valid_loader)-1):\n",
    "                print('EVAL: [{0}/{1}] '\n",
    "                      'Elapsed {remain:s} '\n",
    "                      'Loss: {loss.avg:.4f} '\n",
    "                      .format(step, len(valid_loader),\n",
    "                              remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
    "                              loss=losses))\n",
    "                \n",
    "#     prediction_dict[\"predictions\"] = np.concatenate(preds)\n",
    "        # 修改的部分来处理空的preds列表\n",
    "    if preds:  # 如果preds列表不为空\n",
    "        prediction_dict[\"predictions\"] = np.concatenate(preds)\n",
    "    else:  # 如果preds列表为空\n",
    "        prediction_dict[\"predictions\"] = np.array([])  # 返回空数组或其他默认值\n",
    "\n",
    "    return losses.avg, prediction_dict\n",
    "    return losses.avg, prediction_dict\n",
    "\n",
    "@log_time\n",
    "def train_loop(df, fold,stage =1):\n",
    "    \n",
    "    logging.info(f\"========== Fold: {fold} training ==========\")\n",
    "    \n",
    "    paths.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # ======== SPLIT ==========\n",
    "    train_folds = df[df['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = df[df['fold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    label_cols = ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']\n",
    "    \n",
    "    \n",
    "    # ======== CALCULATE KL DIV ==========\n",
    "    if stage == 2:\n",
    "        # Normalize label distributions\n",
    "        y_train = train_folds[label_cols].values\n",
    "        y_train_normalized = y_train / y_train.sum(axis=1, keepdims=True)\n",
    "        \n",
    "        # Add small value to avoid log(0)\n",
    "        labels = torch.tensor(y_train_normalized, dtype=torch.float) + 1e-5\n",
    "        \n",
    "        # Compute KL Loss with uniform distribution\n",
    "        kl_loss = F.kl_div(torch.log(labels), torch.tensor([1/6]*6, dtype=torch.float), reduction='none').sum(dim=1)\n",
    "        \n",
    "        # Filter based on KL Loss\n",
    "        train_folds = train_folds[kl_loss.numpy() < 5.5].reset_index(drop=True)\n",
    "        logging.info(f\"Filtered training data to {len(train_folds)} samples based on KL Loss < 5.5.\")\n",
    "\n",
    "        \n",
    "        \n",
    "    # ======== DATASETS ==========\n",
    "    train_dataset = CustomDataset(train_folds,config, mode=\"train\", augment=True, specs=all_spectrograms, eeg_specs=all_eegs,wavelets_spectrograms = all_wavelet_spectrograms)\n",
    "    valid_dataset = CustomDataset(valid_folds, config, mode=\"train\", augment=False, specs=all_spectrograms, eeg_specs=all_eegs,wavelets_spectrograms = all_wavelet_spectrograms)\n",
    "    \n",
    "    # ======== DATALOADERS ==========\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=config.BATCH_SIZE_TRAIN,\n",
    "                              shuffle=False,\n",
    "                              num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=config.BATCH_SIZE_VALID,\n",
    "                              shuffle=False,\n",
    "                              num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=False)\n",
    "    \n",
    "    # ======== MODEL ==========\n",
    "    model = CustomModel(config)\n",
    "    model.to(device)\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.1, weight_decay=config.WEIGHT_DECAY)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-3,\n",
    "        epochs=config.EPOCHS,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy=\"cos\",\n",
    "        final_div_factor=100,\n",
    "    )\n",
    "\n",
    "    # ======= LOSS ==========\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    # ====== ITERATE EPOCHS ========\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # ======= TRAIN ==========\n",
    "        avg_train_loss = train_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # ======= EVALUATION ==========\n",
    "        avg_val_loss, prediction_dict = valid_epoch(valid_loader, model, criterion, device)\n",
    "        predictions = prediction_dict[\"predictions\"]\n",
    "        \n",
    "        # ======= SCORING ==========\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        logging.info(f'Epoch {epoch+1} - avg_train_loss: {avg_train_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        \n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            logging.info(f'Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        paths.OUTPUT_DIR / f\"{config.MODEL.replace('/', '_')}_fold_{fold}_best.pth\")\n",
    "\n",
    "    ## TypeError: unsupported operand type(s) for +: 'WindowsPath' and 'str'\n",
    "    # predictions = torch.load(paths.OUTPUT_DIR + f\"/{config.MODEL.replace('/', '_')}_fold_{fold}_best.pth\", \n",
    "    #                          map_location=torch.device('cpu'))['predictions']\n",
    "    paths.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    predictions = torch.load(paths.OUTPUT_DIR / f\"{config.MODEL.replace('/', '_')}_fold_{fold}_best.pth\",\n",
    "                         map_location=torch.device('cpu'))['predictions']\n",
    "\n",
    "    valid_folds[target_preds] = predictions\n",
    "    #+++ 将预测结果添加到valid_folds中，为了区分预测和真实标签，我们给预测的列名添加后缀'_pred'\n",
    "    for i, label in enumerate(label_cols):\n",
    "        valid_folds[label + '_pred'] = predictions[:, i]\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds\n",
    "\n",
    "@log_time\n",
    "def train_loop_full_data(df):\n",
    "    train_dataset = CustomDataset(df, config, mode=\"train\", augment=True,specs=all_spectrograms, eeg_specs=all_eegs,wavelets_spectrograms = all_wavelet_spectrograms)\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=config.BATCH_SIZE_TRAIN,\n",
    "                              shuffle=False,\n",
    "                              num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "    model = CustomModel(config)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.1, weight_decay=config.WEIGHT_DECAY)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-3,\n",
    "        epochs=config.EPOCHS,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy=\"cos\",\n",
    "        final_div_factor=100,\n",
    "    )\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    best_loss = np.inf\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        start_time = time.time()\n",
    "        avg_train_loss = train_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "        elapsed = time.time() - start_time\n",
    "        logging.info(f'Epoch {epoch+1} - avg_train_loss: {avg_train_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        save_path = output_dir / f\"{config.MODEL.replace('/', '_')}_fold_{fold}_best.pth\"\n",
    "        torch.save({'model': model.state_dict(), 'predictions': predictions}, save_path)\n",
    "#         torch.save(\n",
    "#             {'model': model.state_dict()},\n",
    "#             paths.OUTPUT_DIR + f\"/{config.MODEL.replace('/', '_')}_epoch_{epoch}.pth\")\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return \n",
    "\n",
    "@log_time\n",
    "def get_result(oof_df):\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    labels = torch.tensor(oof_df[label_cols].values)\n",
    "    preds = torch.tensor(oof_df[target_preds].values)\n",
    "    preds = F.log_softmax(preds, dim=1)\n",
    "    result = kl_loss(preds, labels)\n",
    "    return result\n",
    "\n",
    "@log_time\n",
    "def preparing_data(df):\n",
    "    train_df = df.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg({\n",
    "        'spectrogram_id':'first',\n",
    "        'spectrogram_label_offset_seconds':'min'\n",
    "    })\n",
    "    train_df.columns = ['spectrogram_id','min']\n",
    "\n",
    "    aux = df.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg({\n",
    "        'spectrogram_label_offset_seconds':'max'\n",
    "    })\n",
    "    train_df['max'] = aux\n",
    "\n",
    "    aux = df.groupby('eeg_id')[['patient_id']].agg('first')\n",
    "    train_df['patient_id'] = aux\n",
    "\n",
    "    aux = df.groupby('eeg_id')[label_cols].agg('sum')\n",
    "    for label in label_cols:\n",
    "        train_df[label] = aux[label].values\n",
    "        \n",
    "    y_data = train_df[label_cols].values\n",
    "    y_data = y_data / y_data.sum(axis=1,keepdims=True)\n",
    "    train_df[label_cols] = y_data\n",
    "\n",
    "    aux = df.groupby('eeg_id')[['expert_consensus']].agg('first')\n",
    "    train_df['target'] = aux\n",
    "\n",
    "    train_df = train_df.reset_index()\n",
    "    return train_df\n",
    "\n",
    "## +++ 计算每个样本的KL散度，并将其作为新列添加到train_df\n",
    "def compute_kl_divergence(data, label_cols):\n",
    "    labels = data[label_cols].values + 1e-5\n",
    "    labels /= labels.sum(axis=1, keepdims=True)\n",
    "    kl_div = torch.nn.functional.kl_div(\n",
    "        torch.log(torch.tensor(labels, dtype=torch.float)),\n",
    "        torch.tensor([[1/6] * 6], dtype=torch.float),\n",
    "        reduction='none'\n",
    "    ).sum(dim=1).numpy()\n",
    "    return kl_div\n",
    "\n",
    "def compute_wavelet_features(signal, wavelet='db4', level=5):\n",
    "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "    # 从小波系数中提取特征而不是直接用小波系数，因为有不规则大小。\n",
    "    features = []\n",
    "    for coeff in coeffs:\n",
    "        features.extend([np.mean(coeff), np.std(coeff)])\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "@log_time\n",
    "def loading_parquet(train_df, config = config, READ_SPEC_FILES = False,READ_EEG_SPEC_FILES = False,wavelet = 'None'):\n",
    "    paths_spectrograms = glob(paths.TRAIN_SPECTROGRAMS + \"*.parquet\")\n",
    "#     paths_spectrograms = glob(str(paths.TRAIN_SPECTROGRAMS / \"*.parquet\"))\n",
    "    print(f'There are {len(paths_spectrograms)} spectrogram parquets in total path')\n",
    "\n",
    "    if READ_SPEC_FILES:    \n",
    "        all_spectrograms = {}\n",
    "        all_wavelet_spectrograms = {}\n",
    "        spectrogram_ids = train_df['spectrogram_id'].unique()\n",
    "        print(f'There are {len(spectrogram_ids)} spectrogram parquets in this training process')\n",
    "        for spec_id in tqdm(spectrogram_ids):\n",
    "        # for file_path in tqdm(paths_spectrograms):\n",
    "            file_path = f\"{paths.TRAIN_SPECTROGRAMS}/{spec_id}.parquet\"\n",
    "            aux = pd.read_parquet(file_path)\n",
    "            spec_arr = aux.fillna(0).values[:, 1:].T.astype(\"float32\")  # (Hz, Time) = (400, 300)\n",
    "            wavelet_features = np.array([compute_wavelet_features(row, wavelet=wavelet) for row in spec_arr])\n",
    "            name = int(file_path.split(\"/\")[-1].split('.')[0])\n",
    "            # all_spectrograms[name] = aux.iloc[:,1:].values  \n",
    "            all_spectrograms[name] = aux.fillna(0).iloc[:,1:].values.astype(\"float32\")\n",
    "            all_wavelet_spectrograms[name] = wavelet_features\n",
    "            del aux\n",
    "            del wavelet_features\n",
    "        os.makedirs(os.path.dirname(paths.PRE_LOADED_SPECTROGRAMS), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(paths.PRE_LOADED_Wavelets), exist_ok=True)\n",
    "        np.save(paths.PRE_LOADED_SPECTROGRAMS, all_spectrograms, allow_pickle=True)\n",
    "        np.save(paths.PRE_LOADED_Wavelets, all_wavelet_spectrograms, allow_pickle=True)\n",
    "    else:\n",
    "        all_spectrograms = np.load(paths.PRE_LOADED_SPECTROGRAMS, allow_pickle=True).item()\n",
    "        all_wavelet_spectrograms = np.load(paths.PRE_LOADED_Wavelets, allow_pickle=True).item()\n",
    "        \n",
    "    if config.VISUALIZE:\n",
    "        idx = np.random.randint(0,len(paths_spectrograms))\n",
    "        spectrogram_path = paths_spectrograms[idx]\n",
    "        plot_spectrogram(spectrogram_path)\n",
    "\n",
    "    # Read EEG Spectrograms\n",
    "    paths_eegs = glob(paths.TRAIN_EEGS + \"*.parquet\")\n",
    "#     paths_eegs = glob(str(paths.TRAIN_EEGS / \"*.parquet\"))\n",
    "    print(f'There are {len(paths_eegs)} EEG spectrograms in total path')\n",
    "    if READ_EEG_SPEC_FILES:\n",
    "        all_eegs = {}\n",
    "        eeg_ids = train_df['eeg_id'].unique()\n",
    "        print(f'There are {len(eeg_ids)} EEG spectrograms in this training path')\n",
    "        for eeg_id in tqdm(eeg_ids):\n",
    "            file_path = f\"{paths.TRAIN_EEGS}/{eeg_id}.parquet\"\n",
    "            eeg_spectrogram =  pd.read_parquet(file_path)\n",
    "            all_eegs[eeg_id] = eeg_spectrogram\n",
    "            del eeg_spectrogram\n",
    "        os.makedirs(os.path.dirname(paths.PRE_LOADED_EEGS), exist_ok=True)\n",
    "        np.save(paths.PRE_LOADED_EEGS, all_eegs, allow_pickle=True)\n",
    "    else:\n",
    "        all_eegs = np.load(paths.PRE_LOADED_EEGS, allow_pickle=True).item()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return all_spectrograms,all_eegs,all_wavelet_spectrograms\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    overall_start_time = time.time()\n",
    "    print(f\"Log file path: {log_filename.absolute()}\")\n",
    "    logging.info('--------------------------------------------------')\n",
    "    logging.info(f'Into loading stage')\n",
    "\n",
    "    target_preds = [x + \"_pred\" for x in ['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote', 'other_vote']]\n",
    "    label_to_num = {'Seizure': 0, 'LPD': 1, 'GPD': 2, 'LRDA': 3, 'GRDA': 4, 'Other':5}\n",
    "    num_to_label = {v: k for k, v in label_to_num.items()}\n",
    "    seed_everything(config.SEED)\n",
    "\n",
    "#     df = pd.read_csv(paths.TRAIN_CSV)\n",
    "    df = pd.read_csv(paths.TRAIN_CSV, nrows=1000)\n",
    "    label_cols = df.columns[-6:]\n",
    "    print(f\"Train cataframe shape is: {df.shape}\")\n",
    "    print(f\"Labels: {list(label_cols)}\")\n",
    "    print(df.head())\n",
    "\n",
    "    #处理train_df，eeg_id,只保留第一个spectrogram_id，min及max spec offset，第一个patient_id等\n",
    "    train_df = preparing_data(df)\n",
    "    print('Train non-overlapp eeg_id shape:', train_df.shape )\n",
    "    print(train_df.head())\n",
    "    train_df.to_csv('./local_train_df.csv', index=False)\n",
    "    \n",
    "    # 添加KL散度列到train_df\n",
    "    train_df['kl_divergence'] = compute_kl_divergence(train_df, label_cols)\n",
    "    print('kl value is ---',train_df[['kl_divergence']])\n",
    "    \n",
    "    logging.info(f'Into loading stage: combine wavelet feature into X')\n",
    "    logging.info(f'Into loading stage: loading single npy from local file')\n",
    "    all_spectrograms,all_eegs,all_wavelet_spectrograms = loading_parquet(train_df, config = config, READ_SPEC_FILES = True,READ_EEG_SPEC_FILES = True,wavelet='db1')\n",
    "    \n",
    "\n",
    "    # Validation \n",
    "    gkf = GroupKFold(n_splits=config.FOLDS)\n",
    "    for fold, (train_index, valid_index) in enumerate(gkf.split(train_df, train_df.target, train_df.patient_id)):\n",
    "        train_df.loc[valid_index, \"fold\"] = int(fold)\n",
    "        \n",
    "    print(train_df.groupby('fold').size()), sep()\n",
    "    print(train_df.head())\n",
    "\n",
    "    logging.info(f'training based on model: efficientnet_b4')\n",
    "    logging.info(f'Feature: without eegs, only specs and wavelets')\n",
    "    train_dataset = CustomDataset(train_df, config, mode=\"train\", \n",
    "                                  specs=all_spectrograms, eeg_specs=all_eegs,wavelets_spectrograms = all_wavelet_spectrograms)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE_TRAIN,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS, pin_memory=True, drop_last=True\n",
    "    )\n",
    "    X, y = train_dataset[0]\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "\n",
    "    if config.VISUALIZE:\n",
    "        ROWS = 2\n",
    "        COLS = 3\n",
    "        for (X, y) in train_loader:\n",
    "            plt.figure(figsize=(20,8))\n",
    "            for row in range(ROWS):\n",
    "                for col in range(COLS):\n",
    "                    plt.subplot(ROWS, COLS, row*COLS + col+1)\n",
    "                    t = y[row*COLS + col]\n",
    "                    img = X[row*COLS + col, :, :, 0]\n",
    "                    mn = img.flatten().min()\n",
    "                    mx = img.flatten().max()\n",
    "                    img = (img-mn)/(mx-mn)\n",
    "                    plt.imshow(img)\n",
    "                    tars = f'[{t[0]:0.2f}'\n",
    "                    for s in t[1:]:\n",
    "                        tars += f', {s:0.2f}'\n",
    "                    eeg = train_df.eeg_id.values[row*config.BATCH_SIZE_TRAIN + row*COLS + col]\n",
    "                    plt.title(f'EEG = {eeg}\\nTarget = {tars}',size=12)\n",
    "                    plt.yticks([])\n",
    "                    plt.ylabel('Frequencies (Hz)',size=14)\n",
    "                    plt.xlabel('Time (sec)',size=16)\n",
    "            plt.show()\n",
    "            break\n",
    "\n",
    "    #dynamic learning rate\n",
    "    EPOCHS = config.EPOCHS\n",
    "    BATCHES = len(train_loader)\n",
    "    steps = []\n",
    "    lrs = []\n",
    "    optim_lrs = []\n",
    "    model = CustomModel(config)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-3,\n",
    "        epochs=config.EPOCHS,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.05,\n",
    "        anneal_strategy=\"cos\",\n",
    "        final_div_factor=100,\n",
    "    )\n",
    "    for epoch in range(EPOCHS):\n",
    "        for batch in range(BATCHES):\n",
    "            scheduler.step()\n",
    "            lrs.append(scheduler.get_last_lr()[0])\n",
    "            steps.append(epoch * BATCHES + batch)\n",
    "\n",
    "    # max_lr = max(lrs)\n",
    "    # min_lr = min(lrs)\n",
    "    # print(f\"Maximum LR: {max_lr} | Minimum LR: {min_lr}\")\n",
    "    # plt.figure()\n",
    "    # plt.plot(steps, lrs, label='OneCycle')\n",
    "    # plt.ticklabel_format(axis='y', style='sci', scilimits=(0,0))\n",
    "    # plt.xlabel(\"Step\")\n",
    "    # plt.ylabel(\"Learning Rate\")\n",
    "    # plt.show()\n",
    "\n",
    "    if not config.TRAIN_FULL_DATA:\n",
    "        oof_df = pd.DataFrame()\n",
    "        results = []  # 存储每个阶段的结果\n",
    "        for fold_id in range(config.FOLDS):\n",
    "            # 第一阶段训练，使用所有数据\n",
    "            print(f\"Starting Stage 1 Training for Fold {fold_id}\")\n",
    "            valid_folds_stage_1 = train_loop(train_df, fold_id, stage=1)\n",
    "            # train_loop返回了字典包含最佳模型的评分和路径\n",
    "            best_model_score_1 = valid_folds_stage_1.get('best_score', None)\n",
    "            best_model_path_1 = valid_folds_stage_1.get('best_model_path', None)\n",
    "\n",
    "            # 第二阶段训练，基于KL散度过滤数据\n",
    "            print(f\"Starting Stage 2 Training for Fold {fold_id}\")\n",
    "            valid_folds_stage_2 = train_loop(train_df, fold_id, stage=2)\n",
    "            best_model_score_2 = valid_folds_stage_2.get('best_score', None)\n",
    "            best_model_path_2 = valid_folds_stage_2.get('best_model_path', None)\n",
    "\n",
    "            results.append((fold_id, best_model_score_1, best_model_path_1, best_model_score_2, best_model_path_2))\n",
    "            logging.info(f\"Fold {fold_id}: Stage 1 - Score: {best_model_score_1}, Path: {best_model_path_1}; Stage 2 - Score: {best_model_score_2}, Path: {best_model_path_2}\")\n",
    "\n",
    "            oof_df = pd.concat([oof_df, valid_folds_stage_2])\n",
    "        # 循环结束后处理oof_df\n",
    "        oof_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "         # 对整体CV结果的打印和日志记录\n",
    "        print(f\"========== CV: {get_result(oof_df)} ==========\")\n",
    "        logging.info(f\"========== CV: {get_result(oof_df)} ==========\")\n",
    "        logging.info(f\"----------------------------------------------------------------------------------\")\n",
    "        # 保存oof_df或进一步分析\n",
    "        oof_df.to_csv(os.path.join(paths.OUTPUT_DIR, 'oof_df.csv'), index=False)\n",
    "    else:\n",
    "        train_loop_full_data(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6fe4c",
   "metadata": {
    "papermill": {
     "duration": 0.06977,
     "end_time": "2024-04-09T01:18:24.083198",
     "exception": false,
     "start_time": "2024-04-09T01:18:24.013428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7469972,
     "sourceId": 59093,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1549.77085,
   "end_time": "2024-04-09T01:18:26.395135",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-09T00:52:36.624285",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "5b2a5d1993d74922b8acbd265842e110": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f0abe6beaf9470d8ec18c9c84b909e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d210cbbd41984b2693f4190ba0190be3",
       "placeholder": "​",
       "style": "IPY_MODEL_de4a3399d61b49f69ffc3f39a05ac3ed",
       "value": " 77.9M/77.9M [00:00&lt;00:00, 96.1MB/s]"
      }
     },
     "60aa0e66c15b4e79909cb99bb37810a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5b2a5d1993d74922b8acbd265842e110",
       "placeholder": "​",
       "style": "IPY_MODEL_9038eeaa1c824ca99dcccc9592bdb3a4",
       "value": "model.safetensors: 100%"
      }
     },
     "7f8deb80c39143ce95e88a0a0d30297a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8960db18a12b4fb391c4f8ed84266882": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9038eeaa1c824ca99dcccc9592bdb3a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "98af0b43eb2545ce9098540561fc6240": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8960db18a12b4fb391c4f8ed84266882",
       "max": 77933206.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7f8deb80c39143ce95e88a0a0d30297a",
       "value": 77933206.0
      }
     },
     "ade114ed90a644c3a46d8b47891b473a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_60aa0e66c15b4e79909cb99bb37810a8",
        "IPY_MODEL_98af0b43eb2545ce9098540561fc6240",
        "IPY_MODEL_5f0abe6beaf9470d8ec18c9c84b909e2"
       ],
       "layout": "IPY_MODEL_c0d8ebff3b9a4da49e98d4cff44f02b0"
      }
     },
     "c0d8ebff3b9a4da49e98d4cff44f02b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d210cbbd41984b2693f4190ba0190be3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de4a3399d61b49f69ffc3f39a05ac3ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
