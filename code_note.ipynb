{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadca8fc",
   "metadata": {},
   "source": [
    "# Requirements of the Project\n",
    "## Tools and Libraries\n",
    "Describe the tools and libraries needed for the project, including specific versions where necessary to ensure compatibility. Common entries might include:\n",
    "\n",
    "    Python: The primary programming language used.\n",
    "    NumPy and Pandas: For data manipulation.\n",
    "    Scikit-Learn: For applying machine learning algorithms.\n",
    "    TensorFlow or PyTorch: If deep learning methods are utilized.\n",
    "    Matplotlib and Seaborn: For plotting and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5a5180",
   "metadata": {},
   "source": [
    "## Spliting small Datasets\n",
    "Instead of using full dataset, we sample small dataset from original data.\n",
    "Loading Data: The dataset is loaded from a CSV file train.csv into a DataFrame df using Pandas.\n",
    "Analyzing Class Distribution: The distribution of classes within the expert_consensus column is analyzed and printed. This helps identify any imbalances among the classes.\n",
    "### Initial Data Distribution\n",
    "\n",
    "| expert_consensus | Count |\n",
    "|------------------|-------|\n",
    "| Seizure          | 20933 |\n",
    "| GRDA             | 18861 |\n",
    "| Other            | 18808 |\n",
    "| GPD              | 16702 |\n",
    "| LRDA             | 16640 |\n",
    "| LPD              | 14856 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3307718",
   "metadata": {},
   "source": [
    "Balancing the Data:\n",
    "The minimum number of samples across all classes (min_samples) is identified, but the code sets a fixed number of desired_samples per class to 50, which might be for ensuring a substantial enough number per class for training purposes. For each class, 50 samples are randomly selected. This selection is done without replacement, ensuring that samples are not repeated within the same class in the balanced dataset.\n",
    "These samples are then added to the balanced_df DataFrame, resulting in a new balanced dataset.\n",
    "\n",
    "Output Balanced Data: \n",
    "The newly balanced data's class distribution is printed to verify that each class now has an equal number of samples. Finally, this balanced dataset is saved to balanced_train.csv, making it available for further processing or model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0caa7355",
   "metadata": {},
   "source": [
    "### Balanced Data Distribution\n",
    "\n",
    "| expert_consensus | Count |\n",
    "|------------------|-------|\n",
    "| Seizure          | 50    |\n",
    "| GPD              | 50    |\n",
    "| LRDA             | 50    |\n",
    "| Other            | 50    |\n",
    "| GRDA             | 50    |\n",
    "| LPD              | 50    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04285336",
   "metadata": {},
   "source": [
    "# Implementation and Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8ea5f3",
   "metadata": {},
   "source": [
    "## Logging set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbb2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_filename = paths.ROOT/'new_version_training_record.log'\n",
    "\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "def log_time(func):\n",
    "    \"\"\"warpper for logging running time\"\"\"\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        logging.info(f\"{func.__name__} took {end_time - start_time:.4f} seconds.\")\n",
    "        print(f\"{func.__name__} took {end_time - start_time:.4f} seconds.\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff94da3",
   "metadata": {},
   "source": [
    "In our code, we implemented a logging system to systematically capture and document the execution times and operational statuses during the training process. The setup includes:\n",
    "\n",
    "Logging Configuration: Logs are generated to include timestamps, log levels, and messages, and are saved in new_version_training_record.log for thorough analysis.\n",
    "Execution Time Monitoring: A decorator, log_time, measures and logs the execution time of critical functions, aiding in the identification and optimization of performance bottlenecks.\n",
    "This methodology ensures precise tracking of model training dynamics, which is crucial for both debugging and optimizing computational efficiency in our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf88d8",
   "metadata": {},
   "source": [
    "##  Pre-processing the Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dfb05b",
   "metadata": {},
   "source": [
    "### Original dataset\n",
    "\n",
    "The example of original spectrogram and eegs are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c7f9e9",
   "metadata": {},
   "source": [
    "spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d21e4",
   "metadata": {},
   "source": [
    "![spectrogram](output/spectrogram.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b301c940",
   "metadata": {},
   "source": [
    "eeg_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b72cf17",
   "metadata": {},
   "source": [
    "![spectrogram](output/eeg_example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92acc16f",
   "metadata": {},
   "source": [
    "More information about this competition can be found in this link:\n",
    "https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/468010\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09bf5f9",
   "metadata": {},
   "source": [
    "### Logarithmic Transformation: \n",
    "Prior to standardization, spectrograms undergo a logarithmic transformation to stabilize the variance across the image. This transformation helps in reducing the skewness of the original data distribution, which is particularly beneficial for spectral data that often spans several orders of magnitude.\n",
    "Data Augmentation: To enhance the robustness of the model against variations in input data, optional augmentation techniques such as horizontal flipping are employed. This helps in simulating a broader set of potential real-world scenarios the model might encounter.\n",
    "\n",
    "### Spectrogram Standardization: \n",
    "Following the logarithmic transformation, each spectrogram is standardized by subtracting the mean and dividing by the standard deviation of its pixel values. This step ensures that the input features have zero mean and unit variance, optimizing conditions for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a211d",
   "metadata": {},
   "source": [
    "import numpy as np \n",
    "def log_and_Standarize(self,img):\n",
    "        # Log transform spectogram\n",
    "            img = np.clip(img, np.exp(-4), np.exp(8))\n",
    "            img = np.log(img)\n",
    "\n",
    "            # Standarize per image\n",
    "            ep = 1e-6\n",
    "            mu = np.nanmean(img.flatten())\n",
    "            std = np.nanstd(img.flatten())\n",
    "            img = (img - mu) / (std + ep)\n",
    "            img = np.nan_to_num(img, nan=0.0)\n",
    "            return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13639b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Wavelet Transform Feature Extraction\n",
    "\n",
    "In our research, we have employed multiple techniques to preprocess and enhance the training dataset to improve the model's ability to process complex signals. In addition to traditional EEG and spectrograms, we have also incorporated wavelet transform techniques to enrich the feature set.\n",
    "\n",
    "Wavelet Transform Feature Extraction:\n",
    "We use the pywt library to perform wavelet decomposition, selecting db1 as the mother wavelet and setting the decomposition level to 5. This step is intended to capture details and trends in the original signal across different scales.\n",
    "Given that wavelet coefficients vary in size and scale, directly using these coefficients might not be suitable for uniform machine learning model inputs. Therefore, we calculate statistical features (mean and standard deviation) from each level's wavelet coefficients, which help maintain consistency in the scale of data inputs.\n",
    "\n",
    "Feature Integration and Standardization:\n",
    "The features derived from wavelet transforms, along with the original spectrogram features, are standardized to format them appropriately for model inputs. This includes applying a logarithmic transformation to smooth the distribution of the data, followed by normalization to ensure the data is on the same scale.\n",
    "The processed wavelet features are merged with other spectrogram features, uniformly resized to the desired resolution (128x256), and integrated into the final input tensor.\n",
    "\n",
    "### Reshaping and Merging Modalities: \n",
    "Spectrograms from different sources—such as EEG and wavelet transforms—are resized and merged into a single tensor. This comprehensive dataset provides a multi-dimensional view of the features, critical for models trained to recognize complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078aca05",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In the training phase of our neural network model, several steps are undertaken to ensure effective learning:\n",
    "\n",
    "### Model Setup: \n",
    "The CustomModel class initializes with a predefined model configuration, leveraging timm.create_model for setting up the underlying architecture, which is a variant of EfficientNet (tf_efficientnet_b0).\n",
    "\n",
    "### Data Preparation: \n",
    "Training data is loaded using the CustomDataset class, which handles data augmentation, multi-modality data integration (spectrograms, EEG, and wavelet spectrograms), and appropriate transformations to prepare batches for the model.\n",
    "\n",
    "## Optimizer and Scheduler:\n",
    "An AdamW optimizer is employed with a learning rate scheduler (OneCycleLR) to adjust learning rates dynamically during training, optimizing convergence rates. The picture of AdamW is shown below.\n",
    "\n",
    "## Training Loop: Using cross-validation and trying different features combinations\n",
    "The training involves processing batches of data, calculating losses using KLDivLoss, and updating model weights. Gradient clipping is applied to avoid exploding gradients. Detailed logs of training progress, including loss metrics and learning rates, are maintained using a custom logging setup.\n",
    "During our investigation, we conducted a series of experiments with the aim of identifying the best combination of features and model parameters for our deep learning models. The experimental setup was configured with 3 epochs and a 3-fold cross-validation strategy to ensure robustness and reliability of the results. In the metric section, we present a summary of the cross-validation results for different combinations of features using two versions of the EfficientNet architecture.\n",
    "\n",
    "## Amp Scaler: \n",
    "Mixed-precision training is facilitated by torch.cuda.amp to accelerate training while conserving memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c5bd8",
   "metadata": {},
   "source": [
    "### Two stages strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fba095",
   "metadata": {},
   "source": [
    "![Learning Rate Schedule](output/lr.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef53a97b",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The model's performance is evaluated using the following steps:\n",
    "\n",
    "### Validation Data Loading: \n",
    "A separate dataset for validation is prepared using the CustomDataset class without data augmentation to evaluate model performance on untouched data.\n",
    "\n",
    "### Evaluation Loop: \n",
    "During evaluation, predictions are made without updating model weights. Losses are computed using the same criterion as in training (KLDivLoss). The softmax function is applied to predictions to convert logits into probabilities.\n",
    "### Metrics Calculation: \n",
    "The evaluation phase logs detailed loss metrics for each batch and overall validation loss, aiding in the assessment of model generalizability.\n",
    "### Model Saving: \n",
    "If the validation loss of the current epoch is lower than previously recorded losses, the model state is saved. This approach ensures that only the best-performing model is retained.\n",
    "These sections highlight the comprehensive strategies employed to train and evaluate the machine learning model, ensuring robustness and accuracy in predictions. Each component of the process is designed to contribute to a deep understanding of the model's performance and areas for improvement.\n",
    "\n",
    "### Features combinations experiment result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77dca87",
   "metadata": {},
   "source": [
    "| Model             | Use Kaggle Spectrograms | Use EEG Spectrograms | Use Wavelet Spectrograms | CV Result   |\n",
    "|-------------------|-------------------------|----------------------|--------------------------|-------------|\n",
    "| **tf_efficientnet_b0** | **True**                    | **False**                | **False**                    | **1.2792**      |\n",
    "| tf_efficientnet_b0 | True                    | True                 | True                     | 1.2816      |\n",
    "| tf_efficientnet_b0 | True                    | True                 | False                    | 1.2915      |\n",
    "| tf_efficientnet_b0 | True                    | False                | True                     | 1.2948      |\n",
    "| tf_efficientnet_b0 | False                   | True                 | True                     | 1.3456      |\n",
    "| tf_efficientnet_b0 | False                   | False                | True                     | 1.3557      |\n",
    "| tf_efficientnet_b0 | False                   | True                 | False                    | 1.3655      |\n",
    "| **efficientnet_b4**    | **True**                    | **False**                | **False**                    | **1.3070**      |\n",
    "| efficientnet_b4    | True                    | False                | True                     | 1.3266      |\n",
    "| efficientnet_b4    | True                    | True                 | True                     | 1.3388      |\n",
    "| efficientnet_b4    | True                    | True                 | False                    | 1.3415      |\n",
    "| efficientnet_b4    | False                   | True                 | True                     | 1.3431      |\n",
    "| efficientnet_b4    | False                   | True                 | False                    | 1.3676      |\n",
    "| efficientnet_b4    | False                   | False                | True                     | 1.3566      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7b1c5",
   "metadata": {},
   "source": [
    "From the results, it is evident that training models using only Kaggle spectrograms (without EEG or wavelet spectrograms) yielded the best cross-validation results, particularly for the tf_efficientnet_b0 and efficientnet_b4 models. Specifically, the tf_efficientnet_b0 model achieved a CV result of 1.2792 when trained solely with Kaggle spectrograms, demonstrating higher performance compared to combinations involving additional features.\n",
    "The outcome measure was computed using a Kullback-Leibler divergence loss function (KLDivLoss), with the predictions subjected to a log-softmax transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffb391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(oof_df):\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    labels = torch.tensor(oof_df[label_cols].values)\n",
    "    preds = torch.tensor(oof_df[target_preds].values)\n",
    "    preds = F.log_softmax(preds, dim=1)\n",
    "    result = kl_loss(preds, labels)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a54412",
   "metadata": {},
   "source": [
    "Based on the experimental trials, we conclude that simpler feature sets may sometimes provide superior predictive performance, potentially due to reduced overfitting and noise. These insights will guide the future direction of our research, focusing on optimizing feature selection and model configuration to enhance model efficacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611f8bc4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a8f2523",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "https://www.kaggle.com/code/alejopaullier/hms-efficientnetb0-pytorch-train\n",
    "https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/discussion/468010\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7469972,
     "sourceId": 59093,
     "sourceType": "competition"
    },
    {
     "datasetId": 4759669,
     "sourceId": 8067308,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24.164645,
   "end_time": "2024-04-08T21:00:31.669933",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-08T21:00:07.505288",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
